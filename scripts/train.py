
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/Trainer.ipynb
"""Note: there is an issue with torchvision's import of Pillow.  If you have an older version of torchvision
or a new version of pillow (>7), and cannot change either, you need to include the fix below"""
import PIL
PIL.PILLOW_VERSION = PIL.__version__

import path
import torch
import torchvision
from functools import partial
import pandas as pd
import numpy as np
import random

from dataloader import *
from models import *
from opt import *
from learner import *
from data_augmentation import *
from metrics import *
from display import display_volume
from loss import *

import time
import pickle
import gc




#root_path      = Path(os.path.expanduser('~'))/r'.data\PulseMapper'
root_path      = Path(r'E:\.data\PulseMapper')
img_folder     = 'Images MC'
gt_folder      = 'Ground Truth'
img_components = ['std', 'mean', 'FAnoise0']
gt_components  = ['rms', 'r2']
extensions     = {'npy'}
img_tfms       = ToTensor()
bs             = 16
lr             = 1e-3
model          = partial(DynamicResnet34Unet, base_model=torchvision.models.resnet34(pretrained=True))
opt_func       = adam_opt(mom=0.9, mom_sqr=0.99, eps=1e-6, wd=1e-2)
loss_func      = torch.nn.MSELoss()
metrics        = [mean, mse, mae, pct_tolerance, abs_tolerance, auc_abs, auc_pct]
epochs         = 10


###lr_sched       = combine_scheds([0.4,0.6],[sched_cos(lr*0.3,lr*0.6),sched_cos(lr*0.6,lr*0.2)])  ##  ©nandinee

#model = torch.load(root_path/'unet_base')
#model=nn.Sequential(model, nn.Conv2d(3,1,1))
#lr_sched=combine_scheds([0.4,0.6],[sched_cos(lr*0.3,lr*0.6),sched_cos(lr*0.6,lr*0.2)])

tfms=[(rotate_batch, 1., {'degrees':(-20,20)}),
      (scale_batch, 0.2, {'x_mag':(0.8, 1.2)}),
      (skew_batch, 0.4,{'mag':(-0.05,0.05)}),
      (reflect_batch, 0.5, {'mirror_h':True}),
      (reflect_batch, 0.5, {'mirror_v':True}),
      (translate_batch, 0.3, {'x_mag':(-0.1,0.1), 'y_mag':(-0.1,0.1)}),
     ]

cbs = [ToDeviceCallback(torch.device('cuda')),
       #NormalizeBatchCallback(norm_x=True),
      ]

cbfs = [ToFloatCallback,
        Recorder,
        StandardizeBatchCallback,
       ]

def get_parameters(new_parameters=None, defaults=None):
    params = {}
    params['root_path']      = root_path
    params['img_folder']     = img_folder
    params['gt_folder']      = gt_folder
    params['img_components'] = img_components
    params['gt_components']  = gt_components
    params['extensions']     = extensions
    params['img_tfms']       = img_tfms
    params['bs']             = bs
    params['lr']             = lr
    params['model']          = model
    params['loss_func']      = loss_func
    params['opt_func']       = opt_func
    params['lr']             = lr
    params['metrics']        = metrics
    params['epochs']         = epochs

    params['tfms']           = tfms
    params['cbs']            = cbs
    params['cbfs']           = cbfs

    if defaults is not None: params.update(defaults)
    if new_parameters is not None: params.update(new_parameters)
    return params

####  ©nandinee
def get_databunch(params, data_split_file=None, save_split=True):  
    #Core parameters
    root_path      = params['root_path']
    img_folder     = params['img_folder']
    gt_folder      = params['gt_folder']
    img_components = params['img_components']
    gt_components  = params['gt_components']
    extensions     = params['extensions']
    img_tfms       = params['img_tfms']
    bs             = params['bs']
    loss_func      = params['loss_func']
    lr             = params['lr']
    metrics        = params['metrics']



    il = MRIImageList.from_files(root_path/img_folder, img_components, extensions, img_tfms=img_tfms)
    
    if data_split_file is None: 
        sd = SplitData.split_by_rdn_user(il, 0.7)
    elif type(data_split_file)==list:
        sd = SplitData.split_by_user_fold(il, data_split_file)
    else:
        sd = pickle.load(open(data_split_file, 'rb'))
    ll = label_by_function(sd, gt_components, relative_labeller, gt_folder, img_tfms)
    data = ll.to_databunch(bs)
    
    ####  ©nandinee
    if save_split:
        return data, sd
    else:
        return data


def get_learner(params, data):
    #Core parameters
    lr             = params['lr']
    model          = params['model']
    loss_func      = params['loss_func']
    metrics        = params['metrics']
    opt_func       = params['opt_func']
    img_components = params['img_components']
    gt_components  = params['gt_components']
    tfms           = params['tfms']
    cbs            = params['cbs'].copy()
    cbfs           = params['cbfs'].copy()
    

    if metrics is not None: cbs += [AvgStatsCallback(metrics)]
    if tfms    is not None: cbs += [BatchTransformCallback(tfms=tfms)]
    
    ####  ©nandinee
    
    #lr_sched=combine_scheds([0.4,0.6],[sched_cos(lr*0.3,lr*0.6),sched_cos(lr*0.6,lr*0.2)])
    ###------ LR Scheduler ---------###
    # lr_sched  =  combine_scheds([0.4,0.6],[sched_cos(lr*0.3,lr*0.6),sched_cos(lr*0.6,lr*0.2)])
    #lr_sched=combine_scheds([0.3,0.3, 0.2, 0.2],[sced_no(lr, lr),sced_no(lr*0.1, lr*0.1), sced_no(lr*0.01, lr*0.01), sced_no(lr*0.001, lr*0.001)])
    lr_sched=combine_scheds([1],[sced_no(lr*1.0, lr*1.0)])
    
    cbs += [ParamScheduler('lr',lr_sched)]

    model = model(c_in = len(img_components), c_out=len(gt_components))

    learn = Learner(model, data, loss_func=loss_func, opt_func=opt_func, cbs=cbs, cbfs=cbfs)
    return learn

def get_output_path(params, base_name='test'):
    root_path      = params['root_path']
    output_folder  = root_path/'output'; output_folder.mkdir(exist_ok=True, parents=True)
    i=0
    while (output_folder/f'{base_name}{i:04}').exists(): i+=1
    return output_folder/f'{base_name}{i:04}'


#export
def stats_row(learn,params):
    metrics   = params.get('metrics', None)
    loss_func = params['loss_func']
    metric_names =['loss']+['metric_'+getattr(x,'__name__','UNK') for x in listify(params['metrics'])] if metrics is not None else ['loss']
    return {stat:value for stat,value in zip(metric_names, learn.avg_stats.valid_stats.avg_stats)}

def params_row(learn, params,param_components = None):
    if param_components is not None: param_info={component:params[component] for component in param_components}
    else: param_info = params
    return param_info

#export
#['img_components','lr','epochs']

####  ©nandinee
def train_models(parameters=None,
                 defaults = None,
                 output_funcs = [stats_row, partial(params_row, param_components=None)],
                 output_images=True,
                 exp_name = None,
                 record_time=True,
                ):
    
    if record_time:     ####  ©nandinee
        start=time.time()

    if parameters is None:
        update_parameters = {'train':{}}
        base_name = 'singles' if exp_name is None else exp_name
    elif isinstance(parameters, (Path, str)):
        update_parameters = read_parameters(parameters)
        base_name = parameters if isinstance(parameters,str) else parameters.name
    else:
        update_parameters = parameters
        base_name = 'generic' if exp_name is None else exp_name

    output_path=None
    df = pd.DataFrame()
    for name, new_parameters in update_parameters:
        random.seed(20)
        params = get_parameters(new_parameters, defaults)
        
        ####  ©nandinee
        data_split_file = params['data_split_file'] if 'data_split_file' in params else None
        
        print(f"Using dataset Split : {data_split_file}")
        data, sd = get_databunch(params, data_split_file=data_split_file)    
        
        learn= get_learner(params, data)

        print (name)
        output_path = get_output_path(params, base_name) if output_path is None else output_path
        output_path.mkdir(exist_ok=True, parents=True)


        epochs        = params['epochs']
        learn.fit(epochs)

        fig = learn.recorder.plot_loss()
        fig.savefig(output_path/(name+'-loss.jpeg'))
        fig.clear()
        plt.close(fig)
        

        if output_images:
            learn.fit(1,cbs=[OutputResultsCallback(output_path/name)])
        fig = learn.show_results()
        fig.savefig(output_path/(name+'.jpeg'))
        fig.clear()
        plt.close(fig)



        torch.save(learn.model, output_path/(name+'.pth'))


        new_row={'name':name}
        for f in output_funcs: new_row= {**new_row, **f(learn,params)}
        df = df.append(new_row, ignore_index=True)
        df.to_csv(output_path/f"{output_path.name}.csv")
        
        
        
        
        
        
        ####  ©nandinee
        
        #### saving data split...
        pickle.dump(sd, open(output_path/(name+'_data_split.pkl'), 'wb') )
        extract_pid = lambda x: x.as_posix().split('/')[2]
        val_pid = [extract_pid(x) for x in sd.valid.items ]
        val_pid = list(set(val_pid))
        train_pid = [extract_pid(x) for x in sd.train.items ]
        train_pid = list(set(train_pid))
        pickle.dump((train_pid, val_pid), open(output_path/(name+'_train_valid_pID.pkl'), 'wb') )
        
        
        fig = learn.recorder.plot_lr()
        fig.savefig(output_path/(name+'-lr.jpeg'))
        fig.clear()
        plt.close(fig)
        
        
        # fig = learn.recorder.plot_epoch_loss()
        # fig.savefig(output_path/(name+'-loss_per_epoch.jpeg'))
        # fig.clear()
        # plt.close(fig)
        
        # fig = learn.recorder.plot_epoch_lr()
        # fig.savefig(output_path/(name+'-lr_per_epoch.jpeg'))
        # fig.clear()
        # plt.close(fig)
        
        if record_time:
            end=time.time()
            d = end - start
            hours, rem = divmod(d, 3600)
            minutes, seconds = divmod(rem, 60)
            time_str="{:0>2}:{:0>2}:{:05.2f}".format(int(hours),int(minutes),seconds)
            print(time_str)
            f= open(output_path/f"{name}_time_record.txt", 'w') 
            f.write(time_str)
            f.close()
        


        del learn.model
        del learn
        
        gc.collect()
        gc.collect()



update_parameters={
    'trial1'     :  {'img_components':['std', 'mean', 'FAnoise0']},
    'trial2'     :  {'img_components':['mean', 'mean', 'mean']},
    'trial3'     :  {'img_components':['std', 'std', 'std']},
    'trial4'     :  {'img_components':['std', 'mean', 'FAnoise0', 'FFT7']},

}

if __name__ == '__main__':
    print('Unit test')
    params = get_parameters()
    data = get_databunch(params)
    learn= get_learner(params, data)
    train_models(update_parameters)
